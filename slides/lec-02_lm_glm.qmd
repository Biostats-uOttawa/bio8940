---
title: "Linear models and generalized linear models"
subtitle: "in R"
author: "Julien Martin"
institute: "BIO 8940"
date: today
format:
  revealjs: 
    width: 1600
    height: 950
    chalkboard: true
    theme: [default]
    css: assets/theme_chalk/whiteboard-blue.css
#    logo: assets/MAD_logo_small_rb.png
    footer: BIO 8940
    show-notes: false
    output-ext: "slides.html"
  html:
    self-contained: true
    number-sections: true
    code-link: true
    format-links: false
    css: assets/css/notes.css
    number-depth: 2
    comments:
      hypothesis: true
    output-ext: notes.html
editor:
  render-on-save: true
engine: knitr
---
# Linear models

```{r}
#| label: setup
#| include: FALSE
#| purl: FALSE
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  dpi = 300,
  fig.showtext = TRUE,
  fig.width = 12,
  fig.height = 6,
#  out.height = "100%",
  dev.args = list(bg = "transparent"),
  cache = TRUE
)
source("assets/theme_chalk/themes_board.r")
theme_set(theme_whiteboard())
```

```{r, include=FALSE}
## load packages
library(car)
library(performance)
library(lmtest)
library(DHARMa)
library(patchwork)
library(tidyverse)
```

## What is a linear regression

### Simple linear regression

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon \\
\epsilon \sim N(0, \sigma^2_{\epsilon})
$$

or in distributional notation $Y \sim N(\beta_0 + \beta_1 x, \sigma^2_{\epsilon})$
 

### General linear model

$$
Y_i = \beta_0 + \beta_1 x_{1i} + \beta_1 x_{2i} + ... + \epsilon \\
\epsilon \sim N(0, \sigma^2_{\epsilon})
$$

and $Y \sim N(\beta_0 + \beta_1 x_{1i} + \beta_1 x_{2i} + ..., \sigma^2)$

## Linear model assumptions

Some are made on the residuals and others on the independent variables.
None are made on the (unconditionned) dependent variable. 

Residuals are assumed to:


* have a mean of zero
* be independent
* be normally distributed
* be homoscedastic


Independent variables are assumed to:

- have a linear relation with Y
- be measured without error
- to be independent from each other



## Maximum likelihood

Technique used for estimating the parameters of a given distribution, using some observed data

\

[For Example:]{.underline}

\

Population is known to follow a “normal distribution” but “mean” and “variance” are unknown, MLE can be used to estimate them using a limited sample of the population.



## Likelihood vs probability

We maximize the likelihood and make inferences on the probability


### Likelihood

$$
L(parameters | data)
$$

How likely it is to get those parameters given the data.



### Probability

$$
P(data | null\ parameters)
$$

Probability to get the data given the null parameters. Or how probable it is to get those data according to the null model.



## Maximum likelihood approach

$$
L(parameters | data) = \prod_{i=1}^{n} f(data_i | parameters)
$$

where f is the probability density function of your model.


Working with product is more painful than with sum, we can take the log:

$$
ln(L(parameters | data)) = \sum_{i=1}^{n} ln(f(data_i | parameters))
$$


Need to solve:

$$
\frac{\delta ln(L(parameters | data)}{\delta parameters} = 0
$$

For multiple regression, the parameters $\beta$s are given by $\beta = (X^T X)^{-1} X^T y$


## Doing linear models in R


Simply use `lm()` function.
It works for everything anova, ancova, t-test.


We will use data of sturgeon measurements at different locations in Canada.


```{r}
dat <- read.csv("data/lm_example.csv")
str(dat)
```



## Fitting a model and checking assumptions

First we load the needed packages for:

* data manipulation: `tidyverse`
* fancy plots: `ggplot2`
* type III anova: `car`
* fancy and nicer visual assumptions checks: `performance`
* formal assumptions tests: `lmtest`


```{r}
#| eval: false
#| code-line-numbers: "4"
library(car)
library(performance)
library(lmtest)
library(tidyverse)
```



## Data exploration

::: {.panel-tabset}

## R Code

```{r plot1, fig.show='hide'}
ggplot(data = dat, aes(x = age, y = fklngth)) +
  facet_grid(. ~ locate) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  stat_smooth(se = FALSE, color = "red") +
  labs(
    y = "Fork length",
    x = "Age"
  )
```


## Plot
```{r plot1, echo=FALSE, fig.show='show'}
```

:::


## Creating log10 transform

```{r}
dat <- dat %>%
  mutate(
    lage = log10(age),
    lfkl = log10(fklngth)
  )
```



## Data exploration: with log

::: {.panel-tabset}

## Code

```{r}
#| label: l10
#| eval: false
ggplot(data = dat, aes(x = lage, y = lfkl)) +
  facet_grid(. ~ locate) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  stat_smooth(se = FALSE, color = "red") +
  labs(
    y = "log 10 Fork length",
    x = "Log 10 Age"
  )
```

## Plot

```{r}
#| label: l10
#| echo: false
```

:::




## Fit the model

```{r}
m1 <- lm(lfkl ~ lage + locate + lage:locate, data = dat)
summary(m1)
```



## Anova for factors

```{r}
Anova(m1, type = 3)
```



## Assumptions (classic)

```{r}
par(mfrow = c(2, 2))
plot(m1)
```

## Assumptions (Nicer)

```{r}
#| eval: false
check_model(m1)
```

```{r}
#| purl: false
#| include: false
pp <- check_model_w(m1)
```

```{r}
#| echo: false
#| purl: false
pp
```

## Formal tests

### Normality of residuals

```{r}
shapiro.test(residuals(m1))
```



## Formal tests

### Heteroscedasticity

```{r}
bptest(m1)
```


## Formal tests

### Linearity

```{r}
resettest(m1, power = 2:3, type = "fitted", data = dat)
```


# Generalized linear models



## Generalized linear models

An extension to **linear models**

GLM expresses the transformed conditional expectation of the dependent variable `Y` as a linear combination of the regression variables `X`

Model has 3 components

- a dependent variable Y with a response distribution to model it: **Gaussian**, **Binomial**, **Bernouilli**, **Poisson**, **negative binomial**, **zero-inflated ...**, **zero-truncated ...**, ...


- linear predictors (or independent variables)
$$
\eta = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k
$$


- a link function such that
$$
E(Y |X) = \mu = g^{-1} (\eta)
$$

<!-- - a structural component or additive expression 
- a link function: $g(\mu)$ -->



## Dependent variable

* when continuous and follows *conditional* normal distribution, called **Linear regression**

* Binary outcomes (success/failure), follows a *Binomial distribution*, called **Logistic regression** 

* Count data (number of events), follows a *Poisson*, called **Poisson regression**



## Classic link functions

* Identity link (form used in linear regression models)

$$
g(\eta) = \mu
$$


* Log link (used when $\mu$ cannot be negative, *e.g.* Poisson data)

$$
g(\eta) = log(\mu)
$$ 


Logit link (used when \mu is bounded between 0 and 1, *e.g.* binary data)

$$
g(\eta) = log\left(\frac{\mu}{1-\mu}\right)
$$


## Linear regression

* Y: continuous, response distribution: Gaussian

$$
Y \sim \mathcal{N}(\mu, \sigma^2_e)
$$

* linear predictor 

$$
\eta = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k\\
$$

* Link function: identity

$$
g(\eta) = \mu \\
\mu = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k
$$



## Logistic regression

* Y: binary or proportion, Response distribution: Binomial or Bernoulli

$$
Y \sim Bernoulli(\pi)
$$

* linear predictor 

$$
\eta = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k\\
$$

* Link function: logit

$$
g(\eta) = ln\left(\frac{\pi}{1-\pi}\right) \\
\pi = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_k X_k)}}
$$



## Poisson regression

* Y: discrete variable (integers), Response distribution: Poisson or Negative binomial

$$
Y \sim \mathcal{P}(\lambda)
$$

* linear predictor 

$$
\eta = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k\\
$$


* Link function: natural logarithm

$$
g(\eta) = ln(\lambda) \\
\lambda = e^{\beta_0 + \beta_1 X_1 + ... + \beta_k X_k}
$$



## Model assumptions


- Easy answer none or really few

- More advanced answer I am not sure, it is complicated

- Just check residuals I as usual

- Technically only 3 assumption:
    - **Variance is a function of the mean specific to the distribution used**

    - observations are independent

    - linear relation on the latent scale

### GLMs do not care if the residual errors are Gaussian as long as the specified mean-variance relationship is satisfied by the data


* what about DHaRMA ? It's complicated


## Choosing a link function

A link function should map the stuctural component from $(-\infty,\infty)$ to the distribution interval (*e.g.* (0,1) for binomial)

So number of link function possible is extremley large.


### Choice of **link** function heavily influenced by field tradiditon


For binomial models

- **logit** assume modelling probability of an observation to be one
- **probit** assume binary outcome from a hidden gaussian variable (*i.e.* threshold model)
- **logit** & **probit** are really similar, both are symmetric but **probit** tapers faster. **logit** coefficient easier to interpret directly
- **cologlog** not-symmetrical


# Logistic regression


## Data

Here is some data to play with from a study on bighorn sheep.

We will look at the relation between reproduction and age

Loading and tweaking the data

```{r}
mouflon0 <- read.csv("data/mouflon.csv")
mouflon <- mouflon0 %>%
  arrange(age) %>%
  mutate(
    reproduction = case_when(
      age >= 13 ~ 0,
      age <= 4 ~ 1,
      .default = reproduction
    )
  )
```

## First plot

::: {.panel-tabset}

## Code
```{r}
#| label: f_plot
#| eval: false
bubble <- data.frame(
  age = rep(2:16, 2),
  reproduction = rep(0:1, each = 15),
  size = c(table(mouflon$age, mouflon$reproduction))
) %>%
  mutate(size = ifelse(size == 0, NA, size))
ggplot(
  bubble,
  aes(x = age, y = reproduction, size = size)
) +
  geom_point(alpha = 0.8) +
  scale_size(range = c(.1, 20), name = "Nb individuals")
```

## Plot

```{r}
#| label: f_plot
#| echo: false
```

:::


## Fitting the logistic regression

```{r}
m1 <- glm(reproduction ~ age, data = mouflon,   family = binomial)
summary(m1)
```

## Checking assumptions

```{r}
simulationOutput <- simulateResiduals(m1)
plot(simulationOutput)
```

## Plotting predictions (latent scale)

plotting the model prediction on the link (latent) scale

```{r}
#| output-location: slide
mouflon$logit_ypred <- 3.19921 - 0.36685 * mouflon$age
plot(logit_ypred ~ jitter(age), mouflon)
points(mouflon$age, mouflon$logit_ypred, col = "red", type = "l", lwd = 2)
```

## Plotting predictions (obs scale)

plotting on the observed scale

```{r}
#| output-location: slide
mouflon$ypred <- exp(mouflon$logit_ypred) / (1 + exp(mouflon$logit_ypred))
ggplot(mouflon, aes(x = age, y = reproduction)) +
  geom_jitter(height = 0.01) +
  geom_line(aes(y=ypred), color = "red")
```


## Plotting predictions (obs scale)

::: {.panel-tabset}

## Code

but it can be much simpler

```{r}
#| label: pred_glm
#| eval: false
dat_predict <- data.frame(
  age = seq(min(mouflon$age), max(mouflon$age), length = 100)
) %>%
  mutate(
    reproduction = predict(m1, type = "response", newdata = .)
  )

ggplot(mouflon, aes(x = age, y = reproduction)) +
  geom_jitter(height = 0.01) +
  geom_line(data = dat_predict, aes(x = age, y = reproduction), color = "red")
```

## Plot

```{r}
#| label: pred_glm
#| echo: false
```

:::


## Your turn

we can do the same things with more complex models

```{r}
m2 <- glm(
  reproduction ~ age + mass_sept + as.factor(sex_lamb) +
    mass_gain + density + temp,
  data = mouflon,
  family = binomial
)
```


## check model

```{r}
#| eval: false
check_model(m2)
```

```{r}
#| include: false
#| purl: false
pp <- check_model_w(m2)
p <- plot(pp)
```

```{r}
#| echo: false
#| purl: false
p
```

## with DHaRMA

```{r}
simulationOutput <- simulateResiduals(m2)
plot(simulationOutput)
```

# Poisson regression

## Data

data on galapagos islands species richness

Fit 3 models:

- model of total number of species
- model of proportion of endemics to total
- model of species density

---

```{r}
gala <- read.csv("data/gala.csv")
plot(Species ~ Area, gala)
```

---

```{r}
plot(Species ~ log(Area), gala)
```

---

```{r}
hist(gala$Species)
```

---

```{r}
modpl <- glm(Species ~ Area + Elevation + Nearest, family = poisson, gala)
summary(modpl)
```

---

```{r}
res <- simulateResiduals(modpl)
testDispersion(res)
```

---

```{r}
c(mean(gala$Species), var(gala$Species))
par(mfrow = c(1, 2))
hist(gala$Species)
hist(rpois(nrow(gala), mean(gala$Species)))
```

---

```{r}
testZeroInflation(res)
```

---

```{r}
par(mfrow = c(2, 2))
plot(modpl)
```

# Happy coding

![](assets/img_l4/unicorn.png){fig-align="center"}
